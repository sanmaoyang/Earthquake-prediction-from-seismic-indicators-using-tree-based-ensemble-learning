{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.ensemble import RUSBoostClassifier\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit,RepeatedKFold,GridSearchCV,RandomizedSearchCV ,StratifiedKFold,KFold\n",
    "from sklearn.svm import LinearSVC , LinearSVR\n",
    "from sklearn.metrics import mean_squared_error , f1_score ,accuracy_score, precision_score, recall_score, f1_score,matthews_corrcoef,auc,roc_auc_score,confusion_matrix\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import scale ,StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2 ,f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import  make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier ,RandomForestRegressor\n",
    "from xgboost import XGBClassifier ,XGBRFRegressor ,XGBRFRegressor\n",
    "#from rotation_forest import RotationTreeClassifier, RotationForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from catboost import CatBoostClassifier,CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\n",
    "\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pathlib import Path\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "import gc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer,Binarizer\n",
    "from datetime import datetime\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import Pool, CatBoostClassifier ,CatBoostRegressor\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from BorutaShap import BorutaShap, load_data\n",
    "from sklearn.utils import shuffle\n",
    "from catboost.utils import eval_metric\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from scipy import stats\n",
    "from scipy.stats import randint\n",
    "from pyts.classification import TimeSeriesForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(input,value):\n",
    "    model = Binarizer(threshold=value)\n",
    "    input = np.array(input)\n",
    "    input = input.reshape(-1,1)\n",
    "    output = model.transform(input)\n",
    "    return output.flatten()\n",
    "\n",
    "def Performance(ytrue,yhat,mag,region):\n",
    "    tn , fp , fn , tp = confusion_matrix(ytrue,yhat).flatten()\n",
    "    mcc = matthews_corrcoef(ytrue,yhat)\n",
    "    f1 = f1_score(ytrue,yhat)\n",
    "    acc = accuracy_score(ytrue,yhat)\n",
    "    recall = recall_score(ytrue,yhat)\n",
    "    precision = precision_score(ytrue,yhat)\n",
    "    output = {\"tn\":tn,\"fp\":fp,\"fn\":fn,\"tp\":tp,\"mcc\":mcc,\"f1\":f1,\"acc\":acc,\"recall\":recall,\"precision\":precision,\"mag\":mag,\"region\":region}\n",
    "    return output\n",
    "\n",
    "scorefunction = make_scorer(matthews_corrcoef, greater_is_better=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def irrel_filter(Data, target):\n",
    "    L=Data.shape[0]\n",
    "    MI=[]\n",
    "    cols=[]\n",
    "    targ_norm = (target-target.min())/(target.max()-target.min())\n",
    "    targ_med=targ_norm.median()\n",
    "    targ_norm_bins = (targ_norm>targ_med).astype(int)\n",
    "    for i in Data.columns:\n",
    "        temp_norm = (Data[i]-Data[i].min())/(Data[i].max()-Data[i].min())\n",
    "        med=temp_norm.median()\n",
    "        temp_norm_bins = (temp_norm>med).astype(int)\n",
    "        U = 2*temp_norm_bins+targ_norm_bins\n",
    "        U0 = U[U==0].shape[0]\n",
    "        U1 = U[U==1].shape[0]\n",
    "        U2 = U[U==2].shape[0]\n",
    "        U3 = U[U==3].shape[0]\n",
    "        \n",
    "        p_targ_0=(U0+U2)/L\n",
    "        p_targ_1=(U1+U3)/L\n",
    "        p_temp_0=(U0+U1)/L\n",
    "        p_temp_1=(U2+U3)/L\n",
    "        \n",
    "        p_targ0_temp0=U0/L\n",
    "        p_targ0_temp1=U2/L\n",
    "        p_targ1_temp0=U1/L\n",
    "        p_targ1_temp1=U3/L\n",
    "        try:\n",
    "            mi=p_targ0_temp0*np.log2(p_targ0_temp0/(p_targ_0*p_temp_0))+p_targ1_temp0*np.log2(p_targ1_temp0/(p_targ_1*p_temp_0))+p_targ0_temp1*np.log2(p_targ0_temp1/(p_targ_0*p_temp_1))+p_targ1_temp1*np.log2(p_targ1_temp1/(p_targ_1*p_temp_1))\n",
    "        except ZeroDivisionError:\n",
    "            mi=np.nan\n",
    "        if np.isnan(mi)==True:\n",
    "            mi=0\n",
    "        if mi<0.001:\n",
    "            mi=-1\n",
    "        MI.append(mi)\n",
    "        if mi==-1:\n",
    "            cols.append(i)\n",
    "    return MI, cols\n",
    "\n",
    "def redundant_filter(Data,MI):\n",
    "    L=Data.shape[0]\n",
    "    RC=np.zeros((1,(Data.shape[1]-1)))\n",
    "    first=np.zeros((1,(Data.shape[1]-1)))\n",
    "    second=np.zeros((1,(Data.shape[1]-1)))\n",
    "    cols=[]\n",
    "    for i in range(Data.shape[1]-1):\n",
    "        first_var=Data.iloc[:,i]\n",
    "        first_norm=((first_var-first_var.min())/(first_var.max()-first_var.min()))\n",
    "        med=first_norm.median()\n",
    "        first_norm_bins=(first_norm>med).astype(int)\n",
    "        for j in range((i+1),(Data.shape[1]-1)):\n",
    "            sec_var=Data.iloc[:,j]\n",
    "            sec_norm=((sec_var-sec_var.min())/(sec_var.max()-sec_var.min()))\n",
    "            med=sec_norm.median()\n",
    "            sec_norm_bins=(sec_norm>med).astype(int)\n",
    "            U=2*sec_norm_bins+first_norm_bins\n",
    "            U0 = U[U==0].shape[0]\n",
    "            U1 = U[U==1].shape[0]\n",
    "            U2 = U[U==2].shape[0]\n",
    "            U3 = U[U==3].shape[0]\n",
    "            \n",
    "            p_first_0=(U0+U2)/L\n",
    "            p_first_1=(U1+U3)/L\n",
    "            p_sec_0=(U0+U1)/L\n",
    "            p_sec_1=(U2+U3)/L\n",
    "\n",
    "            p_first0_sec0=U0/L\n",
    "            p_first0_sec1=U2/L\n",
    "            p_first1_sec0=U1/L\n",
    "            p_first1_sec1=U3/L\n",
    "            try:\n",
    "                Mut_Inf_temp=p_first0_sec0*np.log2(p_first0_sec0/(p_first_0*p_sec_0))+p_first1_sec0*np.log2(p_first1_sec0/(p_first_1*p_sec_0))+p_first0_sec1*np.log2(p_first0_sec1/(p_first_0*p_sec_1))+p_first1_sec1*np.log2(p_first1_sec1/(p_first_1*p_sec_1))\n",
    "            except ZeroDivisionError:\n",
    "                Mut_Inf_temp=np.nan\n",
    "            if np.isnan(Mut_Inf_temp)==True:\n",
    "                Mut_Inf_temp=0\n",
    "            if Mut_Inf_temp>RC[0, i]:\n",
    "                RC[0, i]=Mut_Inf_temp\n",
    "                first[0, i]=i\n",
    "                second[0, i]=j\n",
    "        if RC[0, i]>0.2:\n",
    "            if MI[(first[0, i]).astype(int)]>MI[(second[0, i]).astype(int)]:\n",
    "                drop_ind=second[0, i]\n",
    "            else:\n",
    "                drop_ind=first[0, i]\n",
    "        cols.append(Data.columns[drop_ind.astype(int)])\n",
    "    cols = list(set(cols))\n",
    "    return cols, RC\n",
    "\n",
    "def model_result(model,geodata,thr_mag,regionname):\n",
    "    x,y = geodata.iloc[:,:-1],geodata.iloc[:,-1]\n",
    "    label = threshold(y,thr_mag)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,label,test_size=0.25,shuffle=False)\n",
    "    model.fit(X_train,y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    result = Performance(yhat,y_test,mag=thr_mag,region=regionname)\n",
    "    print(result)\n",
    "    return result\n",
    "    \n",
    "def model_result_mrmr(model,geodata,thr_mag,regionname):\n",
    "    x,y = geodata.iloc[:,:-1],geodata.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.25,shuffle=False)\n",
    "\n",
    "    x,y = X_train.iloc[:,:-1],X_train.iloc[:,-1]\n",
    "    MI1,cols = irrel_filter(X_train,y_train)\n",
    "    cols2,RC = redundant_filter(X_train,MI=MI1)\n",
    "    coldrop = cols2+cols\n",
    "\n",
    "    X_train.drop(coldrop,axis=1,inplace=True)\n",
    "    X_test.drop(coldrop,axis=1,inplace=True)\n",
    "\n",
    "    y_train = threshold(y_train,thr_mag)\n",
    "    y_test = threshold(y_test,thr_mag)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    result = Performance(yhat,y_test,mag=thr_mag,region=regionname)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\12105\\Desktop\\HZ with gap\\raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_data = pd.read_csv(\"SC.csv\")\n",
    "hindu_data = pd.read_csv(\"H.csv\")\n",
    "sc_data.drop_duplicates(keep= \"first\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20387.5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sc_data)*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model,geodata1,thr_mag,regionname):\n",
    "    geodat2 = geodata1.replace(np.inf,np.nan)\n",
    "    geodata = geodat2.dropna()\n",
    "    x,y = geodata.iloc[:,:-1],geodata.iloc[:,-1]\n",
    "    label = threshold(y,thr_mag)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x,label,test_size=0.3,shuffle=False)\n",
    "    model.fit(X_train,y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    result = Performance(yhat,y_test,mag=thr_mag,region=regionname)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tn': 1006, 'fp': 38, 'fn': 44, 'tp': 0, 'mcc': -0.039054693846911764, 'f1': 0.0, 'acc': 0.9246323529411765, 'recall': 0.0, 'precision': 0.0, 'mag': 6, 'region': 'sc'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tn': 1006,\n",
       " 'fp': 38,\n",
       " 'fn': 44,\n",
       " 'tp': 0,\n",
       " 'mcc': -0.039054693846911764,\n",
       " 'f1': 0.0,\n",
       " 'acc': 0.9246323529411765,\n",
       " 'recall': 0.0,\n",
       " 'precision': 0.0,\n",
       " 'mag': 6,\n",
       " 'region': 'sc'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result(TimeSeriesForest(n_jobs=-1),hindu_data,6,\"sc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yang1215",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
